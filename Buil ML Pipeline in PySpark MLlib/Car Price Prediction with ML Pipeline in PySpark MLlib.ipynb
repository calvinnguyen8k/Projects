{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e64323",
   "metadata": {},
   "source": [
    "#### Step 1 (Optional): Install Homebrew\n",
    "If you don’t have Homebrew, here’s the command:\n",
    "\n",
    "- /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n",
    "\n",
    "#### Step 2: Install Java 8\n",
    "Spark requires Java8, and this is where I had to browse Github to find this alternative command:\n",
    "\n",
    "- brew --cask install homebrew/cask-versions/adoptopenjdk8\n",
    "\n",
    "or\n",
    "\n",
    "- brew tap adoptopenjdk/openjdk\n",
    "- brew install --cask adoptopenjdk8\n",
    "\n",
    "or \n",
    "\n",
    "- brew install --cask adoptopenjdk8\n",
    "\n",
    "#### Step 3: Install Scala\n",
    "You probably know it, but Apache-Spark is written in Scala, which is a requirement to run it.\n",
    "\n",
    "- brew install scala\n",
    "\n",
    "#### Step 4: Install Spark\n",
    "We’re almost there. Let’s now install Spark:\n",
    "\n",
    "- brew install apache-spark\n",
    "\n",
    "#### Step 5: Install pySpark\n",
    "You might want to write your Spark code in Python, and pySpark will be useful for that:\n",
    "\n",
    "- pip install pyspark\n",
    "\n",
    "#### Step 6: Modify your bashrc\n",
    "Whether you have bashrc or zshrc, modify your profile with the following commands. Adapt the commands to match your Python path (using which python3) and the folder in which Java has been installed:\n",
    "\n",
    "- export JAVA_HOME=/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\n",
    "- export JRE_HOME=/Library/java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre/\n",
    "- export SPARK_HOME=/usr/local/Cellar/apache-spark/3.0.1/libexec\n",
    "- export PATH=/usr/local/Cellar/apache-spark/3.0.1/bin:$PATH\n",
    "- export PYSPARK_PYTHON=/Users/maelfabien/opt/anaconda3/bin/python\n",
    "\n",
    "Finally, source the profile using:\n",
    "\n",
    "- source .zshrc\n",
    "\n",
    "And you are all set!\n",
    "\n",
    "#### Step 7: Launch a Jupyter Notebook\n",
    "Now, in your Jupyter notebook, you should be able to execute the following commands:\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "n = sc.parallelize([4,10,9,7])\n",
    "\n",
    "n.take(3)\n",
    "\n",
    "\n",
    "\n",
    "#### Option:\n",
    "\n",
    "EDIT To install JDK 8 you need to go to https://www.oracle.com/java/technologies/javase-jdk8-downloads.html (login required)\n",
    "\n",
    "After that I was able to start a Spark context with pyspark.\n",
    "\n",
    "Checking if it works\n",
    "In Python:\n",
    "\n",
    "- from pyspark import SparkContext \n",
    "\n",
    "- sc = SparkContext.getOrCreate() \n",
    "\n",
    "check that it really works by running a job\n",
    "\n",
    "example from http://spark.apache.org/docs/latest/rdd-programming-guide.html#parallelized-collections\n",
    "\n",
    "- data = range(10000) \n",
    "\n",
    "- distData = sc.parallelize(data)\n",
    "\n",
    "- distData.filter(lambda x: not x&1).take(10)\n",
    "\n",
    "Out: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11b82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "# n = sc.parallelize([4,10,9,7])\n",
    "# n.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ebce904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/03 12:18:07 WARN Utils: Your hostname, Calvins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.69 instead (on interface en0)\n",
      "22/02/03 12:18:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/03 12:18:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext \n",
    "sc = SparkContext.getOrCreate() \n",
    "\n",
    "# check that it really works by running a job\n",
    "# example from http://spark.apache.org/docs/latest/rdd-programming-guide.html#parallelized-collections\n",
    "data = range(10000) \n",
    "distData = sc.parallelize(data)\n",
    "distData.filter(lambda x: not x&1).take(10)\n",
    "# Out: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda72489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "#from google.colab import files\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import isnan, when, count, col, lit\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8149c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f350cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.read.csv('data.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6b6f199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Make: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Engine Fuel Type: string (nullable = true)\n",
      " |-- Engine HP: integer (nullable = true)\n",
      " |-- Engine Cylinders: integer (nullable = true)\n",
      " |-- Transmission Type: string (nullable = true)\n",
      " |-- Driven_Wheels: string (nullable = true)\n",
      " |-- Number of Doors: integer (nullable = true)\n",
      " |-- Market Category: string (nullable = true)\n",
      " |-- Vehicle Size: string (nullable = true)\n",
      " |-- Vehicle Style: string (nullable = true)\n",
      " |-- highway MPG: integer (nullable = true)\n",
      " |-- city mpg: integer (nullable = true)\n",
      " |-- Popularity: integer (nullable = true)\n",
      " |-- MSRP: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a406140a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Make</th>\n",
       "      <td>11914</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Acura</td>\n",
       "      <td>Volvo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>11914</td>\n",
       "      <td>745.5822222222222</td>\n",
       "      <td>1490.8280590623795</td>\n",
       "      <td>1 Series</td>\n",
       "      <td>xD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>11914</td>\n",
       "      <td>2010.384337753903</td>\n",
       "      <td>7.5797398875957995</td>\n",
       "      <td>1990</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engine Fuel Type</th>\n",
       "      <td>11911</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>diesel</td>\n",
       "      <td>regular unleaded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engine HP</th>\n",
       "      <td>11845</td>\n",
       "      <td>249.38607007176023</td>\n",
       "      <td>109.19187025917194</td>\n",
       "      <td>55</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engine Cylinders</th>\n",
       "      <td>11884</td>\n",
       "      <td>5.628828677213059</td>\n",
       "      <td>1.78055934824622</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transmission Type</th>\n",
       "      <td>11914</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AUTOMATED_MANUAL</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Driven_Wheels</th>\n",
       "      <td>11914</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>all wheel drive</td>\n",
       "      <td>rear wheel drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of Doors</th>\n",
       "      <td>11908</td>\n",
       "      <td>3.4360933825999327</td>\n",
       "      <td>0.8813153865835529</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Market Category</th>\n",
       "      <td>11914</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Crossover</td>\n",
       "      <td>Performance,Hybrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vehicle Size</th>\n",
       "      <td>11914</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Compact</td>\n",
       "      <td>Midsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vehicle Style</th>\n",
       "      <td>11914</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2dr Hatchback</td>\n",
       "      <td>Wagon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highway MPG</th>\n",
       "      <td>11914</td>\n",
       "      <td>26.637485311398354</td>\n",
       "      <td>8.863000766979422</td>\n",
       "      <td>12</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city mpg</th>\n",
       "      <td>11914</td>\n",
       "      <td>19.73325499412456</td>\n",
       "      <td>8.987798160299237</td>\n",
       "      <td>7</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Popularity</th>\n",
       "      <td>11914</td>\n",
       "      <td>1554.9111969111968</td>\n",
       "      <td>1441.8553466274648</td>\n",
       "      <td>2</td>\n",
       "      <td>5657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSRP</th>\n",
       "      <td>11914</td>\n",
       "      <td>40594.737032063116</td>\n",
       "      <td>60109.10360365422</td>\n",
       "      <td>2000</td>\n",
       "      <td>2065902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                   1                   2  \\\n",
       "summary            count                mean              stddev   \n",
       "Make               11914                None                None   \n",
       "Model              11914   745.5822222222222  1490.8280590623795   \n",
       "Year               11914   2010.384337753903  7.5797398875957995   \n",
       "Engine Fuel Type   11911                None                None   \n",
       "Engine HP          11845  249.38607007176023  109.19187025917194   \n",
       "Engine Cylinders   11884   5.628828677213059    1.78055934824622   \n",
       "Transmission Type  11914                None                None   \n",
       "Driven_Wheels      11914                None                None   \n",
       "Number of Doors    11908  3.4360933825999327  0.8813153865835529   \n",
       "Market Category    11914                None                None   \n",
       "Vehicle Size       11914                None                None   \n",
       "Vehicle Style      11914                None                None   \n",
       "highway MPG        11914  26.637485311398354   8.863000766979422   \n",
       "city mpg           11914   19.73325499412456   8.987798160299237   \n",
       "Popularity         11914  1554.9111969111968  1441.8553466274648   \n",
       "MSRP               11914  40594.737032063116   60109.10360365422   \n",
       "\n",
       "                                  3                   4  \n",
       "summary                         min                 max  \n",
       "Make                          Acura               Volvo  \n",
       "Model                      1 Series                  xD  \n",
       "Year                           1990                2017  \n",
       "Engine Fuel Type             diesel    regular unleaded  \n",
       "Engine HP                        55                1001  \n",
       "Engine Cylinders                  0                  16  \n",
       "Transmission Type  AUTOMATED_MANUAL             UNKNOWN  \n",
       "Driven_Wheels       all wheel drive    rear wheel drive  \n",
       "Number of Doors                   2                   4  \n",
       "Market Category           Crossover  Performance,Hybrid  \n",
       "Vehicle Size                Compact             Midsize  \n",
       "Vehicle Style         2dr Hatchback               Wagon  \n",
       "highway MPG                      12                 354  \n",
       "city mpg                          7                 137  \n",
       "Popularity                        2                5657  \n",
       "MSRP                           2000             2065902  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06643454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(column, value):\n",
    "    return when(column != value, column).otherwise(lit(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47c7c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"Market Category\", replace(col(\"Market Category\"), \"N/A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "758c1183",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Model</th>\n",
       "      <th>Year</th>\n",
       "      <th>Engine Fuel Type</th>\n",
       "      <th>Engine HP</th>\n",
       "      <th>Engine Cylinders</th>\n",
       "      <th>Transmission Type</th>\n",
       "      <th>Driven_Wheels</th>\n",
       "      <th>Number of Doors</th>\n",
       "      <th>Market Category</th>\n",
       "      <th>Vehicle Size</th>\n",
       "      <th>Vehicle Style</th>\n",
       "      <th>highway MPG</th>\n",
       "      <th>city mpg</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>MSRP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3742</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Make  Model  Year  Engine Fuel Type  Engine HP  Engine Cylinders  \\\n",
       "0     0      0     0                 3         69                30   \n",
       "\n",
       "   Transmission Type  Driven_Wheels  Number of Doors  Market Category  \\\n",
       "0                  0              0                6             3742   \n",
       "\n",
       "   Vehicle Size  Vehicle Style  highway MPG  city mpg  Popularity  MSRP  \n",
       "0             0              0            0         0           0     0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).toPandas() #.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c536163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11812, 15)\n"
     ]
    }
   ],
   "source": [
    "data = data.drop('Market Category')\n",
    "data = data.na.drop()\n",
    "print((data.count(), len(data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b5ff8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['Year', 'Engine HP', 'Engine Cylinders', 'Number of Doors', 'highway MPG',\n",
    "                                        'city mpg', 'Popularity'],\n",
    "                           outputCol ='Attributes')\n",
    "\n",
    "regressor = RandomForestRegressor(featuresCol ='Attributes', labelCol='MSRP')\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, regressor])\n",
    "\n",
    "pipeline.write().overwrite().save(\"pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "169156d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Pipeline in PySpark MLlib.ipynb \u001b[34mpipeline\u001b[m\u001b[m\r\n",
      "data.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f4e9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = Pipeline.load(\"pipeline\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "            .addGrid(regressor.numTrees, [100, 500, 1000]) \\\n",
    "            .addGrid(regressor.maxDepth, [5,10]) \\\n",
    "            .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipelineModel,\n",
    "                         estimatorParamMaps = paramGrid,\n",
    "                         evaluator = RegressionEvaluator(labelCol = 'MSRP'),\n",
    "                         numFolds = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "81c5b1c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/03 13:59:45 WARN DAGScheduler: Broadcasting large task binary with size 1503.1 KiB\n",
      "22/02/03 13:59:46 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "22/02/03 13:59:47 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "22/02/03 13:59:48 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "22/02/03 13:59:48 WARN DAGScheduler: Broadcasting large task binary with size 1156.5 KiB\n",
      "22/02/03 13:59:51 WARN DAGScheduler: Broadcasting large task binary with size 1122.0 KiB\n",
      "22/02/03 13:59:52 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/02/03 13:59:56 WARN DAGScheduler: Broadcasting large task binary with size 1122.0 KiB\n",
      "22/02/03 13:59:56 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/02/03 13:59:58 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "22/02/03 13:59:59 WARN DAGScheduler: Broadcasting large task binary with size 1185.5 KiB\n",
      "22/02/03 14:00:00 WARN DAGScheduler: Broadcasting large task binary with size 7.1 MiB\n",
      "22/02/03 14:00:01 WARN DAGScheduler: Broadcasting large task binary with size 1921.8 KiB\n",
      "22/02/03 14:00:02 WARN DAGScheduler: Broadcasting large task binary with size 12.1 MiB\n",
      "22/02/03 14:00:04 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/02/03 14:00:06 WARN DAGScheduler: Broadcasting large task binary with size 19.4 MiB\n",
      "22/02/03 14:00:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/02/03 14:00:11 WARN DAGScheduler: Broadcasting large task binary with size 29.6 MiB\n",
      "22/02/03 14:00:14 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "22/02/03 14:00:21 WARN DAGScheduler: Broadcasting large task binary with size 1066.9 KiB\n",
      "22/02/03 14:00:22 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/02/03 14:00:24 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/02/03 14:00:26 WARN DAGScheduler: Broadcasting large task binary with size 1314.8 KiB\n",
      "22/02/03 14:00:29 WARN DAGScheduler: Broadcasting large task binary with size 1066.9 KiB\n",
      "22/02/03 14:00:31 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/02/03 14:00:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/02/03 14:00:34 WARN DAGScheduler: Broadcasting large task binary with size 1314.8 KiB\n",
      "22/02/03 14:00:35 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "22/02/03 14:00:37 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/02/03 14:00:39 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "22/02/03 14:00:42 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "22/02/03 14:00:45 WARN DAGScheduler: Broadcasting large task binary with size 24.1 MiB\n",
      "22/02/03 14:00:48 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "22/02/03 14:00:53 WARN DAGScheduler: Broadcasting large task binary with size 38.6 MiB\n",
      "22/02/03 14:00:58 WARN DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "22/02/03 14:01:05 WARN DAGScheduler: Broadcasting large task binary with size 59.1 MiB\n",
      "22/02/03 14:01:18 WARN DAGScheduler: Broadcasting large task binary with size 11.3 MiB\n",
      "22/02/03 14:01:31 WARN DAGScheduler: Broadcasting large task binary with size 1445.5 KiB\n",
      "22/02/03 14:01:32 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "22/02/03 14:01:32 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "22/02/03 14:01:33 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "22/02/03 14:01:34 WARN DAGScheduler: Broadcasting large task binary with size 1126.5 KiB\n",
      "22/02/03 14:01:37 WARN DAGScheduler: Broadcasting large task binary with size 1070.0 KiB\n",
      "22/02/03 14:01:38 WARN DAGScheduler: Broadcasting large task binary with size 2025.6 KiB\n",
      "22/02/03 14:01:42 WARN DAGScheduler: Broadcasting large task binary with size 1070.0 KiB\n",
      "22/02/03 14:01:43 WARN DAGScheduler: Broadcasting large task binary with size 2025.6 KiB\n",
      "22/02/03 14:01:44 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "22/02/03 14:01:45 WARN DAGScheduler: Broadcasting large task binary with size 1108.4 KiB\n",
      "22/02/03 14:01:46 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB\n",
      "22/02/03 14:01:47 WARN DAGScheduler: Broadcasting large task binary with size 1824.8 KiB\n",
      "22/02/03 14:01:48 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB\n",
      "22/02/03 14:01:50 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/02/03 14:01:52 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/02/03 14:01:54 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "22/02/03 14:01:57 WARN DAGScheduler: Broadcasting large task binary with size 28.3 MiB\n",
      "22/02/03 14:02:00 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "22/02/03 14:02:06 WARN DAGScheduler: Broadcasting large task binary with size 1043.3 KiB\n",
      "22/02/03 14:02:07 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/02/03 14:02:09 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "22/02/03 14:02:11 WARN DAGScheduler: Broadcasting large task binary with size 1238.2 KiB\n",
      "22/02/03 14:02:15 WARN DAGScheduler: Broadcasting large task binary with size 1043.3 KiB\n",
      "22/02/03 14:02:16 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/02/03 14:02:18 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "22/02/03 14:02:20 WARN DAGScheduler: Broadcasting large task binary with size 1238.2 KiB\n",
      "22/02/03 14:02:20 WARN DAGScheduler: Broadcasting large task binary with size 7.3 MiB\n",
      "22/02/03 14:02:23 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/02/03 14:02:24 WARN DAGScheduler: Broadcasting large task binary with size 13.2 MiB\n",
      "22/02/03 14:02:27 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "22/02/03 14:02:30 WARN DAGScheduler: Broadcasting large task binary with size 22.8 MiB\n",
      "22/02/03 14:02:34 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "22/02/03 14:02:38 WARN DAGScheduler: Broadcasting large task binary with size 36.8 MiB\n",
      "22/02/03 14:02:43 WARN DAGScheduler: Broadcasting large task binary with size 7.9 MiB\n",
      "22/02/03 14:02:49 WARN DAGScheduler: Broadcasting large task binary with size 56.6 MiB\n",
      "22/02/03 14:02:58 WARN DAGScheduler: Broadcasting large task binary with size 10.9 MiB\n",
      "22/02/03 14:03:09 WARN DAGScheduler: Broadcasting large task binary with size 1508.5 KiB\n",
      "22/02/03 14:03:10 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "22/02/03 14:03:11 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "22/02/03 14:03:12 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "22/02/03 14:03:12 WARN DAGScheduler: Broadcasting large task binary with size 1176.4 KiB\n",
      "22/02/03 14:03:15 WARN DAGScheduler: Broadcasting large task binary with size 1094.1 KiB\n",
      "22/02/03 14:03:16 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/02/03 14:03:20 WARN DAGScheduler: Broadcasting large task binary with size 1094.1 KiB\n",
      "22/02/03 14:03:21 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/02/03 14:03:22 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "22/02/03 14:03:23 WARN DAGScheduler: Broadcasting large task binary with size 1174.0 KiB\n",
      "22/02/03 14:03:24 WARN DAGScheduler: Broadcasting large task binary with size 7.0 MiB\n",
      "22/02/03 14:03:25 WARN DAGScheduler: Broadcasting large task binary with size 1926.1 KiB\n",
      "22/02/03 14:03:26 WARN DAGScheduler: Broadcasting large task binary with size 12.0 MiB\n",
      "22/02/03 14:03:28 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "22/02/03 14:03:30 WARN DAGScheduler: Broadcasting large task binary with size 19.4 MiB\n",
      "22/02/03 14:03:32 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "22/02/03 14:03:35 WARN DAGScheduler: Broadcasting large task binary with size 30.0 MiB\n",
      "22/02/03 14:03:38 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "22/02/03 14:03:45 WARN DAGScheduler: Broadcasting large task binary with size 1050.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/03 14:03:46 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/02/03 14:03:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/02/03 14:03:50 WARN DAGScheduler: Broadcasting large task binary with size 1317.4 KiB\n",
      "22/02/03 14:03:53 WARN DAGScheduler: Broadcasting large task binary with size 1050.4 KiB\n",
      "22/02/03 14:03:55 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/02/03 14:03:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/02/03 14:03:58 WARN DAGScheduler: Broadcasting large task binary with size 1317.4 KiB\n",
      "22/02/03 14:03:59 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "22/02/03 14:04:01 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/02/03 14:04:03 WARN DAGScheduler: Broadcasting large task binary with size 14.0 MiB\n",
      "22/02/03 14:04:06 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "22/02/03 14:04:09 WARN DAGScheduler: Broadcasting large task binary with size 24.3 MiB\n",
      "22/02/03 14:04:13 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "22/02/03 14:04:18 WARN DAGScheduler: Broadcasting large task binary with size 39.3 MiB\n",
      "22/02/03 14:04:24 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "22/02/03 14:04:31 WARN DAGScheduler: Broadcasting large task binary with size 60.6 MiB\n",
      "22/02/03 14:04:40 WARN DAGScheduler: Broadcasting large task binary with size 11.7 MiB\n",
      "22/02/03 14:04:52 WARN DAGScheduler: Broadcasting large task binary with size 1047.4 KiB\n",
      "22/02/03 14:04:54 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/02/03 14:04:56 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "22/02/03 14:04:58 WARN DAGScheduler: Broadcasting large task binary with size 1295.6 KiB\n",
      "22/02/03 14:04:59 WARN DAGScheduler: Broadcasting large task binary with size 7.7 MiB\n",
      "22/02/03 14:05:02 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "22/02/03 14:05:04 WARN DAGScheduler: Broadcasting large task binary with size 14.1 MiB\n",
      "22/02/03 14:05:08 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "22/02/03 14:05:11 WARN DAGScheduler: Broadcasting large task binary with size 24.7 MiB\n",
      "22/02/03 14:05:16 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "22/02/03 14:05:21 WARN DAGScheduler: Broadcasting large task binary with size 40.7 MiB\n",
      "22/02/03 14:05:28 WARN DAGScheduler: Broadcasting large task binary with size 9.2 MiB\n",
      "22/02/03 14:05:36 WARN DAGScheduler: Broadcasting large task binary with size 63.7 MiB\n",
      "22/02/03 14:05:53 WARN DAGScheduler: Broadcasting large task binary with size 13.0 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=111)\n",
    "\n",
    "cvModel = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac80a1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorAssembler_57f2dcde93b1\n",
      "RandomForestRegressionModel: uid=RandomForestRegressor_69e98f355331, numTrees=1000, numFeatures=7\n"
     ]
    }
   ],
   "source": [
    "bestModel = cvModel.bestModel\n",
    "for x in range(len(bestModel.stages)):\n",
    "    print(bestModel.stages[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5737a177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "74c33c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "| MSRP|        prediction|\n",
      "+-----+------------------+\n",
      "|29980|30453.328188637977|\n",
      "|30030|30450.048585332726|\n",
      "|30350| 30546.79556016742|\n",
      "|27990|31747.924681006716|\n",
      "|29290|31747.924681006716|\n",
      "|32990|31747.924681006716|\n",
      "| 2912|2913.0022169840636|\n",
      "| 3381| 3414.952109400304|\n",
      "|21050|19029.981156236576|\n",
      "|21600| 18769.52568996837|\n",
      "| 2000|2175.2881398105965|\n",
      "| 2144| 2198.570711117312|\n",
      "|48840| 42597.24318809616|\n",
      "|45015| 42908.84432294007|\n",
      "|49440| 42908.84432294007|\n",
      "|43950| 42757.93691368864|\n",
      "|89000| 48726.59899931966|\n",
      "|40195|  41612.9396332748|\n",
      "|40370| 41784.53005135344|\n",
      "|39270| 41741.38854047442|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred =cvModel.transform(test_data)\n",
    "\n",
    "pred.select('MSRP','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6634073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = RegressionEvaluator(labelCol ='MSRP') #default is RMSE\n",
    "rmse = eval.evaluate(pred)\n",
    "\n",
    "mse = eval.evaluate(pred, {eval.metricName: \"mse\"})\n",
    "mae = eval.evaluate(pred, {eval.metricName: \"mae\"})\n",
    "r_square = eval.evaluate(pred, {eval.metricName: \"r2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec88ce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 21922.188\n",
      "MSE: 480582315.201\n",
      "MAE: 6038.737\n",
      "R2: 0.863\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE: %.3f\" %rmse)\n",
    "print(\"MSE: %.3f\" %mse)\n",
    "print(\"MAE: %.3f\" %mae)\n",
    "print(\"R2: %.3f\" %r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b64b379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RandomForestRegressor in module pyspark.ml.regression object:\n",
      "\n",
      "class RandomForestRegressor(_JavaRegressor, _RandomForestRegressorParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n",
      " |  RandomForestRegressor(*, featuresCol='features', labelCol='label', predictionCol='prediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='variance', subsamplingRate=1.0, seed=None, numTrees=20, featureSubsetStrategy='auto', leafCol='', minWeightFractionPerNode=0.0, weightCol=None, bootstrap=True)\n",
      " |  \n",
      " |  `Random Forest <http://en.wikipedia.org/wiki/Random_forest>`_\n",
      " |  learning algorithm for regression.\n",
      " |  It supports both continuous and categorical features.\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from numpy import allclose\n",
      " |  >>> from pyspark.ml.linalg import Vectors\n",
      " |  >>> df = spark.createDataFrame([\n",
      " |  ...     (1.0, Vectors.dense(1.0)),\n",
      " |  ...     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n",
      " |  >>> rf = RandomForestRegressor(numTrees=2, maxDepth=2)\n",
      " |  >>> rf.getMinWeightFractionPerNode()\n",
      " |  0.0\n",
      " |  >>> rf.setSeed(42)\n",
      " |  RandomForestRegressor...\n",
      " |  >>> model = rf.fit(df)\n",
      " |  >>> model.getBootstrap()\n",
      " |  True\n",
      " |  >>> model.getSeed()\n",
      " |  42\n",
      " |  >>> model.setLeafCol(\"leafId\")\n",
      " |  RandomForestRegressionModel...\n",
      " |  >>> model.featureImportances\n",
      " |  SparseVector(1, {0: 1.0})\n",
      " |  >>> allclose(model.treeWeights, [1.0, 1.0])\n",
      " |  True\n",
      " |  >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n",
      " |  >>> model.predict(test0.head().features)\n",
      " |  0.0\n",
      " |  >>> model.predictLeaf(test0.head().features)\n",
      " |  DenseVector([0.0, 0.0])\n",
      " |  >>> result = model.transform(test0).head()\n",
      " |  >>> result.prediction\n",
      " |  0.0\n",
      " |  >>> result.leafId\n",
      " |  DenseVector([0.0, 0.0])\n",
      " |  >>> model.numFeatures\n",
      " |  1\n",
      " |  >>> model.trees\n",
      " |  [DecisionTreeRegressionModel...depth=..., DecisionTreeRegressionModel...]\n",
      " |  >>> model.getNumTrees\n",
      " |  2\n",
      " |  >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n",
      " |  >>> model.transform(test1).head().prediction\n",
      " |  0.5\n",
      " |  >>> rfr_path = temp_path + \"/rfr\"\n",
      " |  >>> rf.save(rfr_path)\n",
      " |  >>> rf2 = RandomForestRegressor.load(rfr_path)\n",
      " |  >>> rf2.getNumTrees()\n",
      " |  2\n",
      " |  >>> model_path = temp_path + \"/rfr_model\"\n",
      " |  >>> model.save(model_path)\n",
      " |  >>> model2 = RandomForestRegressionModel.load(model_path)\n",
      " |  >>> model.featureImportances == model2.featureImportances\n",
      " |  True\n",
      " |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n",
      " |  True\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      _JavaRegressor\n",
      " |      Regressor\n",
      " |      pyspark.ml.wrapper.JavaPredictor\n",
      " |      pyspark.ml.base.Predictor\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      pyspark.ml.base._PredictorParams\n",
      " |      pyspark.ml.param.shared.HasLabelCol\n",
      " |      pyspark.ml.param.shared.HasFeaturesCol\n",
      " |      pyspark.ml.param.shared.HasPredictionCol\n",
      " |      _RandomForestRegressorParams\n",
      " |      pyspark.ml.tree._RandomForestParams\n",
      " |      pyspark.ml.tree._TreeEnsembleParams\n",
      " |      pyspark.ml.tree._DecisionTreeParams\n",
      " |      pyspark.ml.param.shared.HasCheckpointInterval\n",
      " |      pyspark.ml.param.shared.HasSeed\n",
      " |      pyspark.ml.param.shared.HasWeightCol\n",
      " |      pyspark.ml.tree._TreeRegressorParams\n",
      " |      pyspark.ml.tree._HasVarianceImpurity\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, featuresCol='features', labelCol='label', predictionCol='prediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='variance', subsamplingRate=1.0, seed=None, numTrees=20, featureSubsetStrategy='auto', leafCol='', minWeightFractionPerNode=0.0, weightCol=None, bootstrap=True)\n",
      " |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                  maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10,                  impurity=\"variance\", subsamplingRate=1.0, seed=None, numTrees=20,                  featureSubsetStrategy=\"auto\", leafCol=\", minWeightFractionPerNode=0.0\",                  weightCol=None, bootstrap=True)\n",
      " |  \n",
      " |  setBootstrap(self, value)\n",
      " |      Sets the value of :py:attr:`bootstrap`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setCacheNodeIds(self, value)\n",
      " |      Sets the value of :py:attr:`cacheNodeIds`.\n",
      " |  \n",
      " |  setCheckpointInterval(self, value)\n",
      " |      Sets the value of :py:attr:`checkpointInterval`.\n",
      " |  \n",
      " |  setFeatureSubsetStrategy(self, value)\n",
      " |      Sets the value of :py:attr:`featureSubsetStrategy`.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  setImpurity(self, value)\n",
      " |      Sets the value of :py:attr:`impurity`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setMaxBins(self, value)\n",
      " |      Sets the value of :py:attr:`maxBins`.\n",
      " |  \n",
      " |  setMaxDepth(self, value)\n",
      " |      Sets the value of :py:attr:`maxDepth`.\n",
      " |  \n",
      " |  setMaxMemoryInMB(self, value)\n",
      " |      Sets the value of :py:attr:`maxMemoryInMB`.\n",
      " |  \n",
      " |  setMinInfoGain(self, value)\n",
      " |      Sets the value of :py:attr:`minInfoGain`.\n",
      " |  \n",
      " |  setMinInstancesPerNode(self, value)\n",
      " |      Sets the value of :py:attr:`minInstancesPerNode`.\n",
      " |  \n",
      " |  setMinWeightFractionPerNode(self, value)\n",
      " |      Sets the value of :py:attr:`minWeightFractionPerNode`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setNumTrees(self, value)\n",
      " |      Sets the value of :py:attr:`numTrees`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setParams(self, *, featuresCol='features', labelCol='label', predictionCol='prediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='variance', subsamplingRate=1.0, seed=None, numTrees=20, featureSubsetStrategy='auto', leafCol='', minWeightFractionPerNode=0.0, weightCol=None, bootstrap=True)\n",
      " |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                   maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10,                   impurity=\"variance\", subsamplingRate=1.0, seed=None, numTrees=20,                   featureSubsetStrategy=\"auto\", leafCol=\"\", minWeightFractionPerNode=0.0,                   weightCol=None, bootstrap=True)\n",
      " |      Sets params for linear regression.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setSeed(self, value)\n",
      " |      Sets the value of :py:attr:`seed`.\n",
      " |  \n",
      " |  setSubsamplingRate(self, value)\n",
      " |      Sets the value of :py:attr:`subsamplingRate`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setWeightCol(self, value)\n",
      " |      Sets the value of :py:attr:`weightCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __slotnames__ = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Predictor:\n",
      " |  \n",
      " |  setFeaturesCol(self, value)\n",
      " |      Sets the value of :py:attr:`featuresCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setLabelCol(self, value)\n",
      " |      Sets the value of :py:attr:`labelCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setPredictionCol(self, value)\n",
      " |      Sets the value of :py:attr:`predictionCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  clear(self, param)\n",
      " |      Clears a param from the param map if it has been explicitly set.\n",
      " |  \n",
      " |  copy(self, extra=None)\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          Extra parameters to copy to the new instance\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`JavaParams`\n",
      " |          Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset, params=None)\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      params : dict or list or tuple, optional\n",
      " |          an optional param map that overrides embedded params. If a list/tuple of\n",
      " |          param maps is given, this calls fit on each param map and returns a list of\n",
      " |          models.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n",
      " |          fitted model(s)\n",
      " |  \n",
      " |  fitMultiple(self, dataset, paramMaps)\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      paramMaps : :py:class:`collections.abc.Sequence`\n",
      " |          A Sequence of param maps.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`_FitMultipleIterator`\n",
      " |          A thread safe iterable which contains one model for each param map. Each\n",
      " |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |          using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  getLabelCol(self)\n",
      " |      Gets the value of labelCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  getFeaturesCol(self)\n",
      " |      Gets the value of featuresCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  getPredictionCol(self)\n",
      " |      Gets the value of predictionCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.tree._RandomForestParams:\n",
      " |  \n",
      " |  getBootstrap(self)\n",
      " |      Gets the value of bootstrap or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  getNumTrees(self)\n",
      " |      Gets the value of numTrees or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.tree._RandomForestParams:\n",
      " |  \n",
      " |  bootstrap = Param(parent='undefined', name='bootstrap', doc=...bootstr...\n",
      " |  \n",
      " |  numTrees = Param(parent='undefined', name='numTrees', doc='Number of t...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.tree._TreeEnsembleParams:\n",
      " |  \n",
      " |  getFeatureSubsetStrategy(self)\n",
      " |      Gets the value of featureSubsetStrategy or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  getSubsamplingRate(self)\n",
      " |      Gets the value of subsamplingRate or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.tree._TreeEnsembleParams:\n",
      " |  \n",
      " |  featureSubsetStrategy = Param(parent='undefined', name='featureSubsetS...\n",
      " |  \n",
      " |  subsamplingRate = Param(parent='undefined', name='subsamplingRate'...r...\n",
      " |  \n",
      " |  supportedFeatureSubsetStrategies = ['auto', 'all', 'onethird', 'sqrt',...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.tree._DecisionTreeParams:\n",
      " |  \n",
      " |  getCacheNodeIds(self)\n",
      " |      Gets the value of cacheNodeIds or its default value.\n",
      " |  \n",
      " |  getLeafCol(self)\n",
      " |      Gets the value of leafCol or its default value.\n",
      " |  \n",
      " |  getMaxBins(self)\n",
      " |      Gets the value of maxBins or its default value.\n",
      " |  \n",
      " |  getMaxDepth(self)\n",
      " |      Gets the value of maxDepth or its default value.\n",
      " |  \n",
      " |  getMaxMemoryInMB(self)\n",
      " |      Gets the value of maxMemoryInMB or its default value.\n",
      " |  \n",
      " |  getMinInfoGain(self)\n",
      " |      Gets the value of minInfoGain or its default value.\n",
      " |  \n",
      " |  getMinInstancesPerNode(self)\n",
      " |      Gets the value of minInstancesPerNode or its default value.\n",
      " |  \n",
      " |  getMinWeightFractionPerNode(self)\n",
      " |      Gets the value of minWeightFractionPerNode or its default value.\n",
      " |  \n",
      " |  setLeafCol(self, value)\n",
      " |      Sets the value of :py:attr:`leafCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.tree._DecisionTreeParams:\n",
      " |  \n",
      " |  cacheNodeIds = Param(parent='undefined', name='cacheNodeIds', d...ed o...\n",
      " |  \n",
      " |  leafCol = Param(parent='undefined', name='leafCol', doc='L...ndex of e...\n",
      " |  \n",
      " |  maxBins = Param(parent='undefined', name='maxBins', doc='M...mber of c...\n",
      " |  \n",
      " |  maxDepth = Param(parent='undefined', name='maxDepth', doc='... node + ...\n",
      " |  \n",
      " |  maxMemoryInMB = Param(parent='undefined', name='maxMemoryInMB', ...ati...\n",
      " |  \n",
      " |  minInfoGain = Param(parent='undefined', name='minInfoGain', do...in fo...\n",
      " |  \n",
      " |  minInstancesPerNode = Param(parent='undefined', name='minInstancesPerN...\n",
      " |  \n",
      " |  minWeightFractionPerNode = Param(parent='undefined', name='minWeightFr...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n",
      " |  \n",
      " |  getCheckpointInterval(self)\n",
      " |      Gets the value of checkpointInterval or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n",
      " |  \n",
      " |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n",
      " |  \n",
      " |  getSeed(self)\n",
      " |      Gets the value of seed or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n",
      " |  \n",
      " |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  getWeightCol(self)\n",
      " |      Gets the value of weightCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.tree._HasVarianceImpurity:\n",
      " |  \n",
      " |  getImpurity(self)\n",
      " |      Gets the value of impurity or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.tree._HasVarianceImpurity:\n",
      " |  \n",
      " |  impurity = Param(parent='undefined', name='impurity', doc='...(case-in...\n",
      " |  \n",
      " |  supportedImpurities = ['variance']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param)\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self)\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra=None)\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          extra param values\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param)\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName)\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param)\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName)\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param)\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param)\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param, value)\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self)\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path)\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() from abc.ABCMeta\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path) from abc.ABCMeta\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "86c70a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pred.select('MSRP','prediction').toPandas()\n",
    "test_df['pe'] = (test_df['prediction'] - test_df['MSRP'])/test_df['MSRP']\n",
    "mape = ((test_df['prediction'] - test_df['MSRP'])/test_df['MSRP']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73c907b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 0.056\n",
      "Accuracy: 0.944\n"
     ]
    }
   ],
   "source": [
    "print('MAPE: %.3f' %mape)\n",
    "print('Accuracy: %.3f' %(1-mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd032f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389624dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
